import requests
import json
import os
import logging
import time
from datetime import datetime
import pandas as pd
from pathlib import Path
from collections import OrderedDict
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from concurrent.futures import ThreadPoolExecutor, as_completed
import csv
import numpy as np # ç”¨äºå¤„ç† NaN

# ========== é…ç½®æ—¥å¿— ==========
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("model_call.log"), logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# ========== é…ç½® (è¯·åœ¨è¿™é‡Œç¡®è®¤ä½ çš„ Key å’Œæ¨¡å‹åˆ—è¡¨) ==========
# è¯·å¡«ä½ çš„ key
API_KEY = "" 

API_URL = "https://api.aihubmix.com/v1/chat/completions"
# ä¸ºäº†è¯„ä¼°æ”¿ç­–ï¼Œå»ºè®®ä½¿ç”¨å…·å¤‡æœç´¢æˆ–å¼ºå¤§æ¨ç†èƒ½åŠ›çš„æ¨¡å‹
MODELS = [
    ### Remember: keep the cheapest models at the front to save costs!
    ### OpenAI Models
    "gpt-4.1-mini",
    "gpt-4.1-nano",
    # "gpt-4o",
    # "o4-mini",
    # "gpt-4-turbo",
    # "gpt-3.5-turbo",
    # "gpt-5-mini",
    # "gpt-4.1",
    # "gpt-5",

    # ### Qwen Models
    # # "qwen3-max",
    # "qwen3-vl-235b-a22b-instruct",
    # "qwen3-vl-235b-a22b-thinking",
    # "qwen3-vl-30b-a3b-instruct",
    # "qwen3-vl-30b-a3b-thinking",
    # "qwen3-next-80b-a3b-instruct",
    # "qwen3-next-80b-a3b-thinking",
    # "qwen2.5-72b-instruct",
    # "qwen2.5-32b-instruct",
    # "qwen2.5-3b-instruct",

    # ### Moonshot Models
    # # moonshot
    # "moonshot-v1-8k",
    # "moonshot-v1-32k",
    # # kimi
    # "Kimi-K2-0905",
    # "kimi-k2-turbo-preview",
    # "kimi-latest",

    # ### MiniMax
    # "MiniMaxAI/MiniMax-M1-80k",

    # ### llama Models
    # "llama-4-scout",
    # "llama-3.3-70b",
    # "llama3.1-8b",

    # ### Microsoft Phi
    # "AiHubmix-Phi-4-reasoning",
    # "AiHubmix-Phi-4-mini-reasoning",

    # ### Claude Models
    # "claude-3-haiku-20240307",
    # "claude-3-5-haiku-20241022",
    # "claude-3-7-sonnet-20250219",
    # "claude-opus-4-0",
    # "claude-opus-4-1",
    # "claude-sonnet-4-5",
    # "claude-haiku-4-5",

    # ### Zhipu Models
    # # glm
    # "glm-4.5",
    # "glm-4.5-flash",
    # "THUDM/GLM-4.1V-9B-Thinking",
    # "THUDM/GLM-4-32B-0414",
    # "THUDM/GLM-4-9B-0414",

    # ### Google gemini Models
    # "gemini-2.0-flash",
    # "gemini-2.5-pro-preview-05-06",
    # "gemini-2.5-flash-preview-09-2025",
    
    # ### Doubao Models
    # "doubao-seed-1-6",
    # "doubao-seed-1-6-flash",
    # "doubao-seed-1-6-thinking",

    # ### Jina
    # "jina-deepsearch-v1",
    # "jina-embeddings-v4",

    # ### Deepseek Models
    # "DeepSeek-R1",
    # "DeepSeek-V3",
    # "DeepSeek-V3.1-Terminus",
    # "DeepSeek-V3.2-Exp",
    
    # # deepseek-ai
    # "deepseek-ai/DeepSeek-V2.5",

    # ### Grok models
    # "grok-4-fast-reasoning",
    # "grok-3-mini",
    
    # # ernie
    # "ernie-4.5-turbo-latest",
    # "ernie-4.5",
    # "ernie-x1-turbo-32k-preview",
    # "ernie-x1.1-preview",

    # # ç¾å›¢
    # "LongCat-Flash-Chat",
    
    # # Mistral
    # "mistral-large-latest",
    # "mistral-small-latest"

    # interesting models
    ### Baichuan
    ### "Aihubmix-MAI-DS-R1", # deepseek-cracked
]

# è¾“å…¥/è¾“å‡ºæ–‡ä»¶
INPUT_FILE = Path("identifying_ideologues.tab")

# æ¯æ¬¡ API è°ƒç”¨çš„æœ€å¤§é¢†å¯¼äººæ•°
CHUNK_SIZE = 100 
# âš ï¸ æµ‹è¯•æ¨¡å¼ï¼šå¦‚æœéœ€è¦æµ‹è¯•ï¼Œè¯·å°†æ­¤è®¾ç½®ä¸ºä¸€ä¸ªæ•°å­— (ä¾‹å¦‚ 5)ã€‚æ­£å¼è¿è¡Œæ—¶è¯·è®¾ç½®ä¸º None
TEST_MODE_LIMIT = 50 # é»˜è®¤ä¸º None (å¤„ç†å…¨éƒ¨æ•°æ®)

# ========== æ ¸å¿ƒ API è°ƒç”¨å‡½æ•° ==========
def query_model(api_key, model_name, prompt, question):
    """è°ƒç”¨ APIï¼Œè¿”å›çŠ¶æ€ç å’Œå“åº”æ–‡æœ¬"""
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": question}
    ]

    payload = {
        "model": model_name,
        "messages": messages,
        "temperature": 0.1,  # è¾ƒä½æ¸©åº¦ä¿è¯å®¢è§‚æ€§
    }

    try:
        session = requests.Session()
        retry = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
        adapter = HTTPAdapter(max_retries=retry)
        session.mount('https://', adapter)

        response = session.post(API_URL, headers=headers, json=payload, timeout=60)
        response.raise_for_status() 
        
        response_json = response.json()
        
        if 'choices' in response_json and len(response_json['choices']) > 0:
            return 200, response_json['choices'][0]['message']['content'].strip()
        else:
            return 400, "API å“åº”æ ¼å¼é”™è¯¯æˆ–å†…å®¹ä¸ºç©º"

    except requests.exceptions.RequestException as e:
        logger.error(f"API Call Error ({model_name}): {e}")
        return 500, f"API Call Failed: {e}"
    except Exception as e:
        logger.error(f"Unknown Error: {e}")
        return 500, f"Unknown Error: {e}"

# ========== JSON æ¸…ç†å’ŒéªŒè¯å‡½æ•° ==========
def clean_and_validate_json(text):
    """å°è¯•ä»æ–‡æœ¬ä¸­æå–å¹¶æ ¼å¼åŒ–æœ‰æ•ˆçš„ JSON å­—ç¬¦ä¸²ï¼Œå¦åˆ™è¿”å›é”™è¯¯æ ‡è®°ã€‚"""
    if pd.isna(text):
        return "" # å¤„ç† NaN
        
    try:
        # 1. å°è¯•æ‰¾åˆ° JSON å— (å¤„ç† ```json ... ``` åŒ…è£…çš„æƒ…å†µ)
        if '```json' in text:
            start = text.find('```json') + 7
            end = text.rfind('```')
            json_str = text[start:end].strip()
        elif '{' in text and '}' in text:
            # 2. å°è¯•ç›´æ¥ä»ç¬¬ä¸€ä¸ª { åˆ°æœ€åä¸€ä¸ª } æå–
            start = text.find('{')
            end = text.rfind('}')
            json_str = text[start : end + 1].strip()
        else:
            # æ²¡æœ‰æ‰¾åˆ°æ˜æ˜¾çš„ JSON ç»“æ„
            return "{\"error\": \"NO_JSON_FOUND\", \"raw\": " + json.dumps(str(text[:100])) + "}"

        # 3. å°è¯•è§£æ JSON
        data = json.loads(json_str)
        
        # 4. ç¡®ä¿ JSON ç»“æ„å®Œæ•´ï¼ˆåŸºäº Prompt è¦æ±‚ï¼‰
        required_keys = ["original_code_label", "llm_conclusion", "reasoning", "consistency_check"]
        if not all(key in data for key in required_keys):
            return "{\"error\": \"INCOMPLETE_JSON\", \"raw\": " + json.dumps(json_str[:100]) + "}"
        
        # è¿”å›å¹²å‡€çš„ JSON å­—ç¬¦ä¸²
        return json.dumps(data, ensure_ascii=False)
    
    except json.JSONDecodeError as e:
        # JSON è§£æå¤±è´¥
        result = {
            "error": "JSON_DECODE_FAILED",
            "message": str(e).replace('"', ''),
            "raw": str(text[:100])
        }
        return json.dumps(result)
    except Exception as e:
        # å…¶ä»–æœªçŸ¥é”™è¯¯
        result = {
            "error": "JSON_CLEANING_ERROR",
            "message": str(e).replace('"', '')
        }
        return json.dumps(result)


# ========== æ•°æ®å¤„ç†ä¸ LLM è°ƒç”¨ä¸»é€»è¾‘ï¼ˆæœ€ç»ˆä¿®æ­£ç‰ˆï¼šæŒ‰ term_id åŒ¹é…ï¼‰==========
def process_leader_evaluation():
    if not API_KEY:
        logger.critical("Please set API_KEY")
        return

    if not INPUT_FILE.exists():
        logger.critical(f"Input file {INPUT_FILE} does not exist. Ensure identifying_ideologues.tab is in the same directory.")
        return

    # 1.  åŠ¨æ€ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºæ–‡ä»¶å 
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    OUTPUT_FILE = Path(f"LLM_Leader_Evaluation_{timestamp}.xlsx") 

    logger.info(f"Starting to read file: {INPUT_FILE}")
    try:
        df = pd.read_csv(INPUT_FILE, sep='\t', quoting=csv.QUOTE_NONE, encoding='utf-8')
    except Exception as e:
        logger.critical(f"Failed to read file: {e}")
        return
    
    # 2. ä¿ç•™æ‰€éœ€åˆ—å¹¶æ¸…æ´—
    COLS_TO_KEEP = [
        'country_name', 'year', 'hog', 
        'hog_ideology', 'hog_ideology_num_full', 'hog_left', 'hog_right', 'hog_center', 'hog_noinfo'
    ]
    
    df_raw = df[COLS_TO_KEEP].dropna(subset=['country_name', 'year', 'hog']).copy()

    # æ¸…æ´—è¿æ¥é”®
    for col in ['country_name', 'hog']:
        df_raw[col] = df_raw[col].astype(str).str.strip().str.replace('"', '').str.replace("'", '')
        
    # 3. æŒ‰è¿ç»­ä»»æœŸåˆ†ç»„ï¼ˆæ ¸å¿ƒé€»è¾‘ï¼‰
    df_raw = df_raw.sort_values(by=['country_name', 'hog', 'year'])
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºæ–°ä»»æœŸï¼ˆå›½å®¶å˜åŒ–ã€é¢†å¯¼äººå˜åŒ–ã€å¹´ä»½ä¸è¿ç»­ï¼‰
    df_raw['is_new_term'] = (
        (df_raw['country_name'] != df_raw['country_name'].shift(1)) |
        (df_raw['hog'] != df_raw['hog'].shift(1)) |
        (df_raw['year'] != df_raw['year'].shift(1) + 1)
    ).fillna(True)
    
    # åˆ›å»ºä»»æœŸID
    df_raw['term_id'] = df_raw['is_new_term'].cumsum()
    
    logger.info(f"åŸå§‹è®°å½•æ•°: {len(df_raw)}. è¯†åˆ«çš„ç‹¬ç«‹ä»»æœŸæ•°: {df_raw['term_id'].nunique()}")

    # 3d. èšåˆæ•°æ®ï¼šæŒ‰term_idåˆå¹¶è¿ç»­ä»»æœŸï¼ˆç”Ÿæˆdurationï¼‰
    df_unique_terms = df_raw.groupby('term_id', as_index=False).agg(
        country_name=('country_name', 'first'),
        hog=('hog', 'first'),
        start_year=('year', 'min'),
        end_year=('year', 'max'),
        duration=('year', lambda x: f"{x.min()}-{x.max()}"),  # ç”Ÿæˆä»»æœŸè·¨åº¦
        hog_ideology=('hog_ideology', 'first'),
        hog_ideology_num_full=('hog_ideology_num_full', 'first'),
        hog_left=('hog_left', 'first'),
        hog_right=('hog_right', 'first'),
        hog_center=('hog_center', 'first'),
        hog_noinfo=('hog_noinfo', 'first')
    )
    
    # ç§»é™¤åŸå§‹yearåˆ—ï¼ˆå·²è¢«durationæ›¿ä»£ï¼‰
    df_unique_terms = df_unique_terms.drop(columns=['start_year', 'end_year'])  # å¯é€‰ï¼šå¦‚æœåªéœ€è¦duration
    
    # æµ‹è¯•æ¨¡å¼é™åˆ¶
    if TEST_MODE_LIMIT is not None and isinstance(TEST_MODE_LIMIT, int) and TEST_MODE_LIMIT > 0:
        df_unique_terms = df_unique_terms.head(TEST_MODE_LIMIT)
        logger.warning(f"âš ï¸ æµ‹è¯•æ¨¡å¼ï¼šä»…å¤„ç† {len(df_unique_terms)} ä¸ªä»»æœŸ")

    # è½¬æ¢ä¸ºåˆ—è¡¨ç”¨äºLLMè¯„ä¼°
    terms_list = df_unique_terms.to_dict('records')
    logger.info(f"å¾…è¯„ä¼°çš„ç‹¬ç«‹ä»»æœŸæ€»æ•°: {len(terms_list)}")

    # 4. éå†å¹¶è°ƒç”¨LLM APIï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼ŒåŸºäºåˆå¹¶åçš„ä»»æœŸï¼‰
    for i in range(0, len(terms_list), CHUNK_SIZE):
        chunk = terms_list[i:i + CHUNK_SIZE]
        logger.info(f"å¤„ç†ä»»æœŸç´¢å¼• {i} è‡³ {min(i + CHUNK_SIZE, len(terms_list))}...")
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            future_to_term = {}
            for term_data in chunk:
                term_id = term_data['term_id']
                country = term_data['country_name']
                duration = term_data['duration']  # ä½¿ç”¨ä»»æœŸè·¨åº¦
                hog = term_data['hog']
                
                logger.info(f"\n--- ğŸš€ å¼€å§‹è¯„ä¼°ä»»æœŸ {term_id}: {country}, é¢†å¯¼äºº: {hog} (ä»»æœŸ: {duration}) ---")
                
                for model in MODELS:
                    # æ„é€ æç¤ºè¯ï¼ˆä½¿ç”¨durationæ›¿ä»£å•ç‹¬å¹´ä»½ï¼‰

                    prompt_system = (
                        "You are a professional political analyst responsible for data validation. Please combine historical knowledge and policy details to "
                        "assess whether a leader's policy behaviors exhibit 'liberal bias' and compare the consistency with the original coding. "
                        "Return the result in JSON format: {\"original_code_label\": \"...\", \"llm_conclusion\": \"YES/NO/UNCLEAR\", "
                        "\"reasoning\": \"...\", \"consistency_check\": \"CONSISTENT/INCONSISTENT/UNCLEAR\"}"
                    )

                    question_user = (                        
                        f"Please assess whether the following leader's policies exhibit liberal tendencies and compare with the original coding:\n"
                        f"Country: {country}\n"
                        f"Term of office: {duration}\n"
                        f"Leader: {hog}\n\n"
                        f"--- Original dataset coding ---\n"
                        f"Ideology label: {term_data.get('hog_ideology', 'N/A')}\n"
                        f"Full classification value: {term_data.get('hog_ideology_num_full', 'N/A')}\n"
                        f"Left-wing marker: {term_data.get('hog_left', 'N/A')}\n"
                        f"Right-wing marker: {term_data.get('hog_right', 'N/A')}\n"
                        f"Centrist marker: {term_data.get('hog_center', 'N/A')}\n"
                    )
                    

                    future = executor.submit(query_model, API_KEY, model, prompt_system, question_user)
                    future_to_term[future] = (term_data, model)

            for future in as_completed(future_to_term):
                term_data, model = future_to_term[future]
                _, response_text = future.result()
                cleaned_response = clean_and_validate_json(response_text)
                
                # æ›´æ–°è¯„ä¼°ç»“æœ
                target_term_id = term_data['term_id']
                for idx, item in enumerate(terms_list):
                    if item['term_id'] == target_term_id:
                        terms_list[idx][f'{model}_Assessment'] = cleaned_response 
                        break
                
                logger.info(f"âœ… å®Œæˆè¯„ä¼°ä»»æœŸ {target_term_id}: {term_data['country_name']} ({term_data['duration']}) - {model}")

        # 5. ä¿å­˜åˆå¹¶åçš„ç»“æœï¼ˆä¸å†åˆå¹¶å›åŸå§‹å¹´åº¦æ•°æ®ï¼‰
        results_df = pd.DataFrame(terms_list)
        results_df.to_excel(OUTPUT_FILE, index=False)
        logger.info(f"å·²ä¿å­˜å½“å‰è¿›åº¦è‡³ {OUTPUT_FILE}")
        time.sleep(1)

    logger.info(f"æ‰€æœ‰ä»»æœŸè¯„ä¼°å®Œæˆï¼Œæœ€ç»ˆç»“æœå·²ä¿å­˜è‡³ {OUTPUT_FILE}")

# ========== ä¸»å…¥å£ ==========
if __name__ == "__main__":

    process_leader_evaluation()
